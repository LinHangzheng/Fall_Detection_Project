{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import imageio\n",
    "import tensorflow as tf\n",
    "from numpy.random import seed\n",
    "seed(1)\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "from matplotlib import pyplot as plt\n",
    "import os\n",
    "import h5py\n",
    "import scipy.io as sio\n",
    "import cv2\n",
    "import glob\n",
    "import gc\n",
    "from tensorflow import keras as keras\n",
    "from keras.models import load_model, Model, Sequential\n",
    "from keras.layers import (Input, Conv2D, MaxPooling2D, Flatten,\n",
    "                          Activation, Dense, Dropout, ZeroPadding2D)\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers.normalization import BatchNormalization \n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras import backend as K\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.model_selection import KFold, StratifiedShuffleSplit\n",
    "from keras.layers.advanced_activations import ELU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "\n",
    "# CHANGE THESE VARIABLES ---\n",
    "data_folder = 'E:/JuniorYearSpring/ECE397/fall_detection/URFD_opticalflow/Falls/fall_fall-09'\n",
    "mean_file = 'E:/JuniorYearSpring/ECE397/fall_detection/flow_mean.mat'\n",
    "vgg_16_weights = 'E:/JuniorYearSpring/ECE397/fall_detection/weights.h5'\n",
    "save_features = False\n",
    "save_plots = True\n",
    "\n",
    "# Set to 'True' if you want to restore a previous trained models\n",
    "# Training is skipped and test is done\n",
    "use_checkpoint = False # Set to True or False\n",
    "# --------------------------\n",
    "\n",
    "best_model_path = 'models/'\n",
    "plots_folder = 'plots/'\n",
    "checkpoint_path = 'models/fold_'\n",
    "\n",
    "features_file = 'features_urfd_tf.h5'\n",
    "labels_file = 'labels_urfd_tf.h5'\n",
    "features_key = 'features'\n",
    "labels_key = 'labels'\n",
    "\n",
    "L = 10\n",
    "num_features = 4096\n",
    "batch_norm = True\n",
    "learning_rate = 0.0001\n",
    "mini_batch_size = 0\n",
    "weight_0 = 1.0\n",
    "epochs = 1000\n",
    "\n",
    "# Name of the experiment\n",
    "exp = 'urfd_lr{}_batchs{}_batchnorm{}_w0_{}'.format(learning_rate,\n",
    "                                               mini_batch_size,\n",
    "                                               batch_norm,\n",
    "                                               weight_0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline \n",
    "I = tf.read_file(data_folder+\"/flow_x_00001.jpg\")\n",
    "I = tf.image.decode_jpeg(I, channels=3) \n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    I=sess.run((I))\n",
    "#     print (I.shape) \n",
    "#     print (I)\n",
    "    plt.imshow(I) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_info(case, metrics, save, history):\n",
    "    '''\n",
    "    Function to create plots for train and validation loss and accuracy\n",
    "    Input:\n",
    "    * case: name for the plot, an 'accuracy.png' or 'loss.png' \n",
    "        will be concatenated after the name.\n",
    "    * metrics: list of metrics to store: 'loss' and/or 'accuracy'\n",
    "    * save: boolean to store the plots or only show them.\n",
    "    * history: History object returned by the Keras fit function.\n",
    "    '''\n",
    "    plt.ioff()\n",
    "    if 'accuracy' in metrics:     \n",
    "        fig = plt.figure()\n",
    "        plt.plot(history['acc'])\n",
    "        plt.plot(history['val_acc'])\n",
    "        plt.title('model accuracy')\n",
    "        plt.ylabel('accuracy')\n",
    "        plt.xlabel('epoch')\n",
    "        plt.legend(['train', 'val'], loc='upper left')\n",
    "        if save == True:\n",
    "            plt.savefig(case + 'accuracy.png')\n",
    "            plt.gcf().clear()\n",
    "        else:\n",
    "            plt.show()\n",
    "        plt.close(fig)\n",
    "\n",
    "    # summarize history for loss\n",
    "    if 'loss' in metrics:\n",
    "        fig = plt.figure()\n",
    "        plt.plot(history['loss'])\n",
    "        plt.plot(history['val_loss'])\n",
    "        plt.title('model loss')\n",
    "        plt.ylabel('loss')\n",
    "        plt.xlabel('epoch')\n",
    "        #plt.ylim(1e-3, 1e-2)\n",
    "        plt.yscale(\"log\")\n",
    "        plt.legend(['train', 'val'], loc='upper left')\n",
    "        if save == True:\n",
    "            plt.savefig(case + 'loss.png')\n",
    "            plt.gcf().clear()\n",
    "        else:\n",
    "            plt.show()\n",
    "        plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator(list1, lits2):\n",
    "    '''\n",
    "    Auxiliar generator: returns the ith element of both given list with\n",
    "         each call to next() \n",
    "    '''\n",
    "    for x,y in zip(list1,lits2):\n",
    "        yield x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveFeatures(feature_extractor,\n",
    "                 features_file,\n",
    "                 labels_file,\n",
    "                 features_key, \n",
    "                 labels_key):\n",
    "    '''\n",
    "    Function to load the optical flow stacks, do a feed-forward through the\n",
    "         feature extractor (VGG16) and\n",
    "    store the output feature vectors in the file 'features_file' and the \n",
    "        labels in 'labels_file'.\n",
    "    Input:\n",
    "    * feature_extractor: model VGG16 until the fc6 layer.\n",
    "    * features_file: path to the hdf5 file where the extracted features are\n",
    "         going to be stored\n",
    "    * labels_file: path to the hdf5 file where the labels of the features\n",
    "         are going to be stored\n",
    "    * features_key: name of the key for the hdf5 file to store the features\n",
    "    * labels_key: name of the key for the hdf5 file to store the labels\n",
    "    '''\n",
    "    \n",
    "    class0 = 'Falls'\n",
    "    class1 = 'NotFalls'     \n",
    "\n",
    "    # Load the mean file to subtract to the images\n",
    "    d = sio.loadmat(mean_file)\n",
    "    flow_mean = d['image_mean']\n",
    "\n",
    "    # Fill the folders and classes arrays with all the paths to the data\n",
    "    folders, classes = [], []\n",
    "    fall_videos = [f for f in os.listdir(data_folder + class0) \n",
    "                        if os.path.isdir(os.path.join(data_folder + class0, f))]\n",
    "    fall_videos.sort()\n",
    "    for fall_video in fall_videos:\n",
    "        x_images = glob.glob(data_folder + class0 + '/' + fall_video\n",
    "                                 + '/flow_x*.jpg')\n",
    "        if int(len(x_images)) >= 10:\n",
    "            folders.append(data_folder + class0 + '/' + fall_video)\n",
    "            classes.append(0)\n",
    "\n",
    "    not_fall_videos = [f for f in os.listdir(data_folder + class1) \n",
    "                        if os.path.isdir(os.path.join(data_folder + class1, f))]\n",
    "    not_fall_videos.sort()\n",
    "    for not_fall_video in not_fall_videos:\n",
    "        x_images = glob.glob(data_folder + class1 + '/' + not_fall_video\n",
    "                                 + '/flow_x*.jpg')\n",
    "        if int(len(x_images)) >= 10:\n",
    "            folders.append(data_folder + class1 + '/' + not_fall_video)\n",
    "            classes.append(1)\n",
    "\n",
    "    # Total amount of stacks, with sliding window = num_images-L+1\n",
    "    nb_total_stacks = 0\n",
    "    for folder in folders:\n",
    "        x_images = glob.glob(folder + '/flow_x*.jpg')\n",
    "        nb_total_stacks += len(x_images)-L+1\n",
    "    \n",
    "    # File to store the extracted features and datasets to store them\n",
    "    # IMPORTANT NOTE: 'w' mode totally erases previous data\n",
    "    h5features = h5py.File(features_file,'w')\n",
    "    h5labels = h5py.File(labels_file,'w')\n",
    "    dataset_features = h5features.create_dataset(features_key,\n",
    "                         shape=(nb_total_stacks, num_features),\n",
    "                         dtype='float64')\n",
    "    dataset_labels = h5labels.create_dataset(labels_key,\n",
    "                         shape=(nb_total_stacks, 1),\n",
    "                         dtype='float64')  \n",
    "    cont = 0\n",
    "    \n",
    "    for folder, label in zip(folders, classes):\n",
    "        x_images = glob.glob(folder + '/flow_x*.jpg')\n",
    "        x_images.sort()\n",
    "        y_images = glob.glob(folder + '/flow_y*.jpg')\n",
    "        y_images.sort()\n",
    "        nb_stacks = len(x_images)-L+1\n",
    "        # Here nb_stacks optical flow stacks will be stored\n",
    "        flow = np.zeros(shape=(224,224,2*L,nb_stacks), dtype=np.float64)\n",
    "        gen = generator(x_images,y_images)\n",
    "        for i in range(len(x_images)):\n",
    "            flow_x_file, flow_y_file = gen.next()\n",
    "            img_x = cv2.imread(flow_x_file, cv2.IMREAD_GRAYSCALE)\n",
    "            img_y = cv2.imread(flow_y_file, cv2.IMREAD_GRAYSCALE)\n",
    "            # Assign an image i to the jth stack in the kth position, but also\n",
    "            # in the j+1th stack in the k+1th position and so on        \n",
    "            # (for sliding window) \n",
    "            for s in list(reversed(range(min(10,i+1)))):\n",
    "                if i-s < nb_stacks:\n",
    "                    flow[:,:,2*s,  i-s] = img_x\n",
    "                    flow[:,:,2*s+1,i-s] = img_y\n",
    "            del img_x,img_y\n",
    "            gc.collect()\n",
    "            \n",
    "        # Subtract mean\n",
    "        flow = flow - np.tile(flow_mean[...,np.newaxis],\n",
    "                              (1, 1, 1, flow.shape[3]))\n",
    "        flow = np.transpose(flow, (3, 0, 1, 2)) \n",
    "        predictions = np.zeros((flow.shape[0], num_features), dtype=np.float64)\n",
    "        truth = np.zeros((flow.shape[0], 1), dtype=np.float64)\n",
    "        # Process each stack: do the feed-forward pass and store\n",
    "        # in the hdf5 file the output\n",
    "        for i in range(flow.shape[0]):\n",
    "            prediction = feature_extractor.predict(\n",
    "                                        np.expand_dims(flow[i, ...],0))\n",
    "            predictions[i, ...] = prediction\n",
    "            truth[i] = label\n",
    "        dataset_features[cont:cont+flow.shape[0],:] = predictions\n",
    "        dataset_labels[cont:cont+flow.shape[0],:] = truth\n",
    "        cont += flow.shape[0]\n",
    "    h5features.close()\n",
    "    h5labels.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_video(feature_extractor, video_path, ground_truth):\n",
    "    # Load the mean file to subtract to the images\n",
    "    d = sio.loadmat(mean_file)\n",
    "    flow_mean = d['image_mean']\n",
    "    \n",
    "    x_images = glob.glob(video_path + '/flow_x*.jpg')\n",
    "    x_images.sort()\n",
    "    y_images = glob.glob(video_path + '/flow_y*.jpg')\n",
    "    y_images.sort()\n",
    "    nb_stacks = len(x_images)-L+1\n",
    "    # Here nb_stacks optical flow stacks will be stored\n",
    "    print (L,nb_stacks)\n",
    "    flow = np.zeros(shape=(224,224,2*L,nb_stacks), dtype=np.float64)\n",
    "    gen = generator(x_images,y_images)\n",
    "    for i in range(len(x_images)):\n",
    "        flow_x_file, flow_y_file = next(gen)\n",
    "        img_x = cv2.imread(flow_x_file, cv2.IMREAD_GRAYSCALE)\n",
    "        img_y = cv2.imread(flow_y_file, cv2.IMREAD_GRAYSCALE)\n",
    "        # Assign an image i to the jth stack in the kth position, but also\n",
    "        # in the j+1th stack in the k+1th position and so on\n",
    "        # (for sliding window) \n",
    "        for s in list(reversed(range(min(10,i+1)))):\n",
    "            if i-s < nb_stacks:\n",
    "                flow[:,:,2*s,  i-s] = img_x\n",
    "                flow[:,:,2*s+1,i-s] = img_y\n",
    "        del img_x,img_y\n",
    "        gc.collect()\n",
    "    flow = flow - np.tile(flow_mean[...,np.newaxis], (1, 1, 1, flow.shape[3]))\n",
    "    flow = np.transpose(flow, (3, 0, 1, 2)) \n",
    "    predictions = np.zeros((flow.shape[0], num_features), dtype=np.float64)\n",
    "    truth = np.zeros((flow.shape[0], 1), dtype=np.float64)\n",
    "    # Process each stack: do the feed-forward pass\n",
    "    for i in range(flow.shape[0]):\n",
    "        prediction = feature_extractor.predict(np.expand_dims(flow[i, ...],0))\n",
    "        predictions[i, ...] = prediction\n",
    "        truth[i] = ground_truth\n",
    "    return predictions, truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # ========================================================================\n",
    "    # VGG-16 ARCHITECTURE\n",
    "    # ========================================================================\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(ZeroPadding2D((1, 1), input_shape=(224, 224, 20)))\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu', name='conv1_1'))\n",
    "    model.add(ZeroPadding2D((1, 1)))\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu', name='conv1_2'))\n",
    "    model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "\n",
    "    model.add(ZeroPadding2D((1, 1)))\n",
    "    model.add(Conv2D(128, (3, 3), activation='relu', name='conv2_1'))\n",
    "    model.add(ZeroPadding2D((1, 1)))\n",
    "    model.add(Conv2D(128, (3, 3), activation='relu', name='conv2_2'))\n",
    "    model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "\n",
    "    model.add(ZeroPadding2D((1, 1)))\n",
    "    model.add(Conv2D(256, (3, 3), activation='relu', name='conv3_1'))\n",
    "    model.add(ZeroPadding2D((1, 1)))\n",
    "    model.add(Conv2D(256, (3, 3), activation='relu', name='conv3_2'))\n",
    "    model.add(ZeroPadding2D((1, 1)))\n",
    "    model.add(Conv2D(256, (3, 3), activation='relu', name='conv3_3'))\n",
    "    model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "\n",
    "    model.add(ZeroPadding2D((1, 1)))\n",
    "    model.add(Conv2D(512, (3, 3), activation='relu', name='conv4_1'))\n",
    "    model.add(ZeroPadding2D((1, 1)))\n",
    "    model.add(Conv2D(512, (3, 3), activation='relu', name='conv4_2'))\n",
    "    model.add(ZeroPadding2D((1, 1)))\n",
    "    model.add(Conv2D(512, (3, 3), activation='relu', name='conv4_3'))\n",
    "    model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "\n",
    "    model.add(ZeroPadding2D((1, 1)))\n",
    "    model.add(Conv2D(512, (3, 3), activation='relu', name='conv5_1'))\n",
    "    model.add(ZeroPadding2D((1, 1)))\n",
    "    model.add(Conv2D(512, (3, 3), activation='relu', name='conv5_2'))\n",
    "    model.add(ZeroPadding2D((1, 1)))\n",
    "    model.add(Conv2D(512, (3, 3), activation='relu', name='conv5_3'))\n",
    "    model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(num_features, name='fc6', kernel_initializer='glorot_uniform'))\n",
    "    \n",
    "    # ========================================================================\n",
    "    # WEIGHT INITIALIZATION\n",
    "    # ========================================================================\n",
    "    layerscaffe = ['conv1_1', 'conv1_2', 'conv2_1', 'conv2_2', 'conv3_1',\n",
    "                   'conv3_2', 'conv3_3', 'conv4_1', 'conv4_2', 'conv4_3',\n",
    "                   'conv5_1', 'conv5_2', 'conv5_3', 'fc6', 'fc7', 'fc8']\n",
    "    h5 = h5py.File(vgg_16_weights, 'r')\n",
    "    \n",
    "    layer_dict = dict([(layer.name, layer) for layer in model.layers])\n",
    "\n",
    "    # Copy the weights stored in the 'vgg_16_weights' file to the\n",
    "    # feature extractor part of the VGG16\n",
    "    for layer in layerscaffe[:-3]:\n",
    "        w2, b2 = h5['data'][layer]['0'], h5['data'][layer]['1']\n",
    "        #w2 = np.transpose(np.asarray(w2), (0,1,2,3))\n",
    "        #w2 = w2[:, :, ::-1, ::-1]\n",
    "        w2 = np.transpose(np.asarray(w2), (2,3,1,0))\n",
    "        w2 = w2[::-1, ::-1, :, :]\n",
    "        b2 = np.asarray(b2)\n",
    "        #layer_dict[layer].W.set_value(w2)\n",
    "        #layer_dict[layer].b.set_value(b2)\n",
    "        layer_dict[layer].set_weights((w2, b2))\n",
    "    #sys.exit()\n",
    "    # Copy the weights of the first fully-connected layer (fc6)\n",
    "    layer = layerscaffe[-3]\n",
    "    w2, b2 = h5['data'][layer]['0'], h5['data'][layer]['1']\n",
    "    w2 = np.transpose(np.asarray(w2), (1,0))\n",
    "    b2 = np.asarray(b2)\n",
    "    #layer_dict[layer].W.set_value(w2)\n",
    "    #layer_dict[layer].b.set_value(b2)\n",
    "    layer_dict[layer].set_weights((w2, b2))\n",
    "\n",
    "    # ========================================================================\n",
    "    # FEATURE EXTRACTION\n",
    "    # ========================================================================\n",
    "    # if save_features:\n",
    "    #    saveFeatures(model, features_file,\n",
    "    #                labels_file, features_key,\n",
    "    #                labels_key)\n",
    "    # ========================================================================\n",
    "    # TRAINING/Testing\n",
    "    # ========================================================================    \n",
    "    #adam = Adam(lr=learning_rate, beta_1=0.9, beta_2=0.999,\n",
    "    #            epsilon=1e-08)\n",
    "    #model.compile(optimizer=adam, loss='categorical_crossentropy',\n",
    "    #              metrics=['accuracy'])\n",
    "   \n",
    "    do_training = False  \n",
    "    do_testing = True \n",
    "    compute_metrics = True\n",
    "    threshold = 0.5\n",
    "    ground_truth = 0\n",
    "    fold_best_model_path = 'E:/JuniorYearSpring/ECE397/urfd_fold_5.h5'\n",
    "\n",
    "    if do_testing:\n",
    "        X2,truth = test_video(model, data_folder, ground_truth)\n",
    "        print('Model loaded from checkpoint')\n",
    "        classifier = load_model(fold_best_model_path)\n",
    "        if compute_metrics:\n",
    "            predicted = classifier.predict(np.asarray(X2))\n",
    "            print(predicted)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0419 16:33:33.735523  2368 deprecation_wrapper.py:119] From c:\\users\\cc\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0419 16:33:33.738516  2368 deprecation_wrapper.py:119] From c:\\users\\cc\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0419 16:33:33.750483  2368 deprecation_wrapper.py:119] From c:\\users\\cc\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W0419 16:33:33.796356  2368 deprecation_wrapper.py:119] From c:\\users\\cc\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4267: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "W0419 16:33:34.013476  2368 deprecation_wrapper.py:119] From c:\\users\\cc\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "W0419 16:33:34.013476  2368 deprecation_wrapper.py:119] From c:\\users\\cc\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0419 16:33:44.925175  2368 deprecation.py:506] From c:\\users\\cc\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "W0419 16:33:44.926172  2368 nn_ops.py:4224] Large dropout rate: 0.9 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded from checkpoint\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0419 16:33:45.057986  2368 nn_ops.py:4224] Large dropout rate: 0.8 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "W0419 16:33:45.448377  2368 deprecation_wrapper.py:119] From c:\\users\\cc\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\keras\\optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W0419 16:33:45.458384  2368 deprecation.py:323] From c:\\users\\cc\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\ops\\nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "c:\\users\\cc\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\keras\\engine\\saving.py:350: UserWarning: Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n",
      "  warnings.warn('Error in loading the saved optimizer '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[6.2099099e-04]\n",
      " [1.6871095e-04]\n",
      " [1.1217594e-04]\n",
      " [3.0708313e-04]\n",
      " [1.0728836e-04]\n",
      " [4.3421984e-05]\n",
      " [5.8442354e-05]\n",
      " [4.0113926e-05]\n",
      " [2.9295683e-05]\n",
      " [5.0663948e-06]\n",
      " [2.6524067e-06]\n",
      " [5.3226948e-05]\n",
      " [4.3809414e-06]\n",
      " [2.1159649e-06]\n",
      " [1.7881393e-07]\n",
      " [8.9406967e-08]\n",
      " [3.9265281e-08]\n",
      " [1.4305599e-06]\n",
      " [3.6479159e-06]\n",
      " [1.4194123e-06]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n               for i in range(len(predicted)):\\n                   if predicted[i] < threshold:\\n                       predicted[i] = 0\\n                   else:\\n                       predicted[i] = 1\\n               # Array of predictions 0/1\\n               predicted = np.asarray(predicted).astype(int)   \\n               # Compute metrics and print them\\n               cm = confusion_matrix(truth, predicted,labels=[0,1])\\n               tp = cm[0][0]\\n               fn = cm[0][1]\\n               fp = cm[1][0]\\n               tn = cm[1][1]\\n               tpr = tp/float(tp+fn)\\n               fpr = fp/float(fp+tn)\\n               fnr = fn/float(fn+tp)\\n               tnr = tn/float(tn+fp)\\n               precision = tp/float(tp+fp)\\n               recall = tp/float(tp+fn)\\n               specificity = tn/float(tn+fp)\\n               f1 = 2*float(precision*recall)/float(precision+recall)\\n               accuracy = accuracy_score(_y2, predicted)\\n               \\n         \\n               print(\\'FOLD {} results:\\'.format(fold_number))\\n               print(\\'TP: {}, TN: {}, FP: {}, FN: {}\\'.format(tp,tn,fp,fn))\\n               print(\\'TPR: {}, TNR: {}, FPR: {}, FNR: {}\\'.format(\\n                                                        tpr,tnr,fpr,fnr))   \\n               print(\\'Sensitivity/Recall: {}\\'.format(recall))\\n               print(\\'Specificity: {}\\'.format(specificity))\\n               print(\\'Precision: {}\\'.format(precision))\\n               print(\\'F1-measure: {}\\'.format(f1))\\n               print(\\'Accuracy: {}\\'.format(accuracy))\\n               fold_number += 1\\n               \\n               # Store the metrics for this epoch\\n               sensitivities.append(tp/float(tp+fn))\\n               specificities.append(tn/float(tn+fp))\\n               fars.append(fpr)\\n               mdrs.append(fnr)\\n               accuracies.append(accuracy)\\n\\n    if do_training:\\n        h5features = h5py.File(features_file, \\'r\\')\\n        h5labels = h5py.File(labels_file, \\'r\\')\\n        \\n        # X_full will contain all the feature vectors extracted\\n        # from optical flow images\\n        X_full = h5features[features_key]\\n        _y_full = np.asarray(h5labels[labels_key])\\n        \\n        zeroes_full = np.asarray(np.where(_y_full==0)[0])\\n        ones_full = np.asarray(np.where(_y_full==1)[0])\\n        zeroes_full.sort()\\n        ones_full.sort()\\n        \\n        # Use a 5 fold cross-validation\\n        kf_falls = KFold(n_splits=5, shuffle=True)\\n        kf_falls.get_n_splits(X_full[zeroes_full, ...])\\n        \\n        kf_nofalls = KFold(n_splits=5, shuffle=True)\\n        kf_nofalls.get_n_splits(X_full[ones_full, ...])        \\n\\n        sensitivities = []\\n        specificities = []\\n        fars = []\\n        mdrs = []\\n        accuracies = []\\n            \\n        fold_number = 1\\n        # CROSS-VALIDATION: Stratified partition of the dataset into\\n        # train/test sets\\n        for ((train_index_falls, test_index_falls),\\n            (train_index_nofalls, test_index_nofalls)) in zip(\\n                 kf_falls.split(X_full[zeroes_full, ...]),\\n                 kf_nofalls.split(X_full[ones_full, ...])\\n            ):\\n\\n            train_index_falls = np.asarray(train_index_falls)\\n            test_index_falls = np.asarray(test_index_falls)\\n            train_index_nofalls = np.asarray(train_index_nofalls)\\n            test_index_nofalls = np.asarray(test_index_nofalls)\\n\\n            X = np.concatenate((X_full[zeroes_full, ...][train_index_falls, ...],\\n                                X_full[ones_full, ...][train_index_nofalls, ...]))\\n            _y = np.concatenate((_y_full[zeroes_full, ...][train_index_falls, ...],\\n                                 _y_full[ones_full, ...][train_index_nofalls, ...]))\\n            X2 = np.concatenate((X_full[zeroes_full, ...][test_index_falls, ...],\\n                                 X_full[ones_full, ...][test_index_nofalls, ...]))\\n            _y2 = np.concatenate((_y_full[zeroes_full, ...][test_index_falls, ...],\\n                                  _y_full[ones_full, ...][test_index_nofalls, ...]))   \\n\\n            # Create a validation subset from the training set\\n            val_size = 100\\n            zeroes = np.asarray(np.where(_y==0)[0])\\n            ones = np.asarray(np.where(_y==1)[0])\\n            \\n            zeroes.sort()\\n            ones.sort()\\n\\n            trainval_split_0 = StratifiedShuffleSplit(n_splits=1,\\n                                                   test_size=int(val_size/2),\\n                                                   random_state=7)\\n            indices_0 = trainval_split_0.split(X[zeroes,...],\\n                                             np.argmax(_y[zeroes,...], 1))\\n            trainval_split_1 = StratifiedShuffleSplit(n_splits=1,\\n                                                   test_size=int(val_size/2),\\n                                                   random_state=7)\\n            indices_1 = trainval_split_1.split(X[ones,...],\\n                                             np.argmax(_y[ones,...], 1))\\n            train_indices_0, val_indices_0 = next(indices_0)\\n            train_indices_1, val_indices_1 = next(indices_1)\\n\\n            X_train = np.concatenate([X[zeroes,...][train_indices_0,...],\\n                                      X[ones,...][train_indices_1,...]],axis=0)\\n            y_train = np.concatenate([_y[zeroes,...][train_indices_0,...],\\n                                      _y[ones,...][train_indices_1,...]],axis=0)\\n            X_val = np.concatenate([X[zeroes,...][val_indices_0,...],\\n                                      X[ones,...][val_indices_1,...]],axis=0)\\n            y_val = np.concatenate([_y[zeroes,...][val_indices_0,...],\\n                                      _y[ones,...][val_indices_1,...]],axis=0)\\n           \\n\\n            # Balance the number of positive and negative samples so that\\n            # there is the same amount of each of them\\n            all0 = np.asarray(np.where(y_train==0)[0])\\n            all1 = np.asarray(np.where(y_train==1)[0])  \\n            \\n            if len(all0) < len(all1):\\n                all1 = np.random.choice(all1, len(all0), replace=False)\\n            else:\\n                all0 = np.random.choice(all0, len(all1), replace=False)\\n            allin = np.concatenate((all0.flatten(),all1.flatten()))\\n            allin.sort()\\n            X_train = X_train[allin,...]\\n            y_train = y_train[allin]\\n            \\n            all0 = np.asarray(np.where(y_train==0)[0])\\n            all1 = np.asarray(np.where(y_train==1)[0])\\n            \\n            # ==================== CLASSIFIER ========================\\n            extracted_features = Input(shape=(num_features,),\\n                                       dtype=\\'float32\\', name=\\'input\\')\\n            if batch_norm:\\n                x = BatchNormalization(axis=-1, momentum=0.99,\\n                                       epsilon=0.001)(extracted_features)\\n                x = Activation(\\'relu\\')(x)\\n            else:\\n                x = ELU(alpha=1.0)(extracted_features)\\n           \\n            x = Dropout(0.9)(x)\\n            x = Dense(4096, name=\\'fc2\\', kernel_initializer=\\'glorot_uniform\\')(x)\\n            if batch_norm:\\n                x = BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001)(x)\\n                x = Activation(\\'relu\\')(x)\\n            else:\\n                x = ELU(alpha=1.0)(x)\\n            x = Dropout(0.8)(x)\\n            x = Dense(1, name=\\'predictions\\',\\n                        kernel_initializer=\\'glorot_uniform\\')(x)\\n            x = Activation(\\'sigmoid\\')(x)\\n            \\n            classifier = Model(input=extracted_features,\\n                               output=x, name=\\'classifier\\')\\n            fold_best_model_path = best_model_path + \\'urfd_fold_{}.h5\\'.format(\\n                                                                fold_number)\\n            classifier.compile(optimizer=adam, loss=\\'binary_crossentropy\\',\\n                               metrics=[\\'accuracy\\'])\\n\\n            if not use_checkpoint:\\n                # ==================== TRAINING ========================     \\n                # weighting of each class: only the fall class gets\\n                # a different weight\\n                class_weight = {0: weight_0, 1: 1}\\n\\n                # callback definition\\n                metric = \\'val_loss\\'\\n                e = EarlyStopping(monitor=metric, min_delta=0, patience=100,\\n                                  mode=\\'auto\\')\\n                c = ModelCheckpoint(fold_best_model_path, monitor=metric,\\n                                    save_best_only=True,\\n                                    save_weights_only=False, mode=\\'auto\\')\\n                callbacks = [e, c]\\n\\n                # Batch training\\n                if mini_batch_size == 0:\\n                        history = classifier.fit(X_train, y_train, \\n                                                validation_data=(X_val,y_val),\\n                                                batch_size=X_train.shape[0],\\n                                                nb_epoch=epochs,\\n                                                shuffle=\\'batch\\',\\n                                                class_weight=class_weight,\\n                                                callbacks=callbacks)\\n                else:\\n                        history = classifier.fit(X_train, y_train, \\n                                                validation_data=(X_val,y_val),\\n                                                batch_size=mini_batch_size,\\n                                                nb_epoch=epochs,\\n                                                shuffle=\\'batch\\',\\n                                                class_weight=class_weight,\\n                                                callbacks=callbacks)\\n\\n                plot_training_info(plots_folder + exp, [\\'accuracy\\', \\'loss\\'],\\n                                   save_plots, history.history)\\n\\n                classifier = load_model(fold_best_model_path)\\n\\n                # Use full training set (training+validation)\\n                X_train = np.concatenate((X_train, X_val), axis=0)\\n                y_train = np.concatenate((y_train, y_val), axis=0)\\n\\n                if mini_batch_size == 0:\\n                        history = classifier.fit(X_train, y_train, \\n                                                batch_size=X_train.shape[0],\\n                                                nb_epoch=1,\\n                                                shuffle=\\'batch\\',\\n                                                class_weight=class_weight)\\n                else:\\n                        history = classifier.fit(X_train, y_train, \\n                                                batch_size=mini_batch_size,\\n                                                nb_epoch=1,\\n                                                shuffle=\\'batch\\',\\n                                                class_weight=class_weight)\\n\\n                classifier.save(fold_best_model_path)\\n\\n            # ==================== EVALUATION ========================     \\n            \\n            # Load best model\\n            print(\\'Model loaded from checkpoint\\')\\n            classifier = load_model(fold_best_model_path)\\n   \\n            if compute_metrics:\\n               predicted = classifier.predict(np.asarray(X2))\\n               for i in range(len(predicted)):\\n                   if predicted[i] < threshold:\\n                       predicted[i] = 0\\n                   else:\\n                       predicted[i] = 1\\n               # Array of predictions 0/1\\n               predicted = np.asarray(predicted).astype(int)   \\n               # Compute metrics and print them\\n               cm = confusion_matrix(_y2, predicted,labels=[0,1])\\n               tp = cm[0][0]\\n               fn = cm[0][1]\\n               fp = cm[1][0]\\n               tn = cm[1][1]\\n               tpr = tp/float(tp+fn)\\n               fpr = fp/float(fp+tn)\\n               fnr = fn/float(fn+tp)\\n               tnr = tn/float(tn+fp)\\n               precision = tp/float(tp+fp)\\n               recall = tp/float(tp+fn)\\n               specificity = tn/float(tn+fp)\\n               f1 = 2*float(precision*recall)/float(precision+recall)\\n               accuracy = accuracy_score(_y2, predicted)\\n               \\n               print(\\'FOLD {} results:\\'.format(fold_number))\\n               print(\\'TP: {}, TN: {}, FP: {}, FN: {}\\'.format(tp,tn,fp,fn))\\n               print(\\'TPR: {}, TNR: {}, FPR: {}, FNR: {}\\'.format(\\n                                                        tpr,tnr,fpr,fnr))   \\n               print(\\'Sensitivity/Recall: {}\\'.format(recall))\\n               print(\\'Specificity: {}\\'.format(specificity))\\n               print(\\'Precision: {}\\'.format(precision))\\n               print(\\'F1-measure: {}\\'.format(f1))\\n               print(\\'Accuracy: {}\\'.format(accuracy))\\n               fold_number += 1\\n               \\n               # Store the metrics for this epoch\\n               sensitivities.append(tp/float(tp+fn))\\n               specificities.append(tn/float(tn+fp))\\n               fars.append(fpr)\\n               mdrs.append(fnr)\\n               accuracies.append(accuracy)\\n\\n        print(\\'5-FOLD CROSS-VALIDATION RESULTS ===================\\')\\n        print(\"Sensitivity: %.2f%% (+/- %.2f%%)\" % (np.mean(sensitivities),\\n                                                         np.std(sensitivities)))\\n        print(\"Specificity: %.2f%% (+/- %.2f%%)\" % (np.mean(specificities),\\n                                                         np.std(specificities)))\\n        print(\"FAR: %.2f%% (+/- %.2f%%)\" % (np.mean(fars), np.std(fars)))\\n        print(\"MDR: %.2f%% (+/- %.2f%%)\" % (np.mean(mdrs), np.std(mdrs)))\\n        print(\"Accuracy: %.2f%% (+/- %.2f%%)\" % (np.mean(accuracies),\\n                                                         np.std(accuracies)))\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "if __name__ == '__main__':\n",
    "    if not os.path.exists(best_model_path):\n",
    "        os.makedirs(best_model_path)\n",
    "    if not os.path.exists(plots_folder):\n",
    "        os.makedirs(plots_folder)\n",
    "        \n",
    "    main()\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "               for i in range(len(predicted)):\n",
    "                   if predicted[i] < threshold:\n",
    "                       predicted[i] = 0\n",
    "                   else:\n",
    "                       predicted[i] = 1\n",
    "               # Array of predictions 0/1\n",
    "               predicted = np.asarray(predicted).astype(int)   \n",
    "               # Compute metrics and print them\n",
    "               cm = confusion_matrix(truth, predicted,labels=[0,1])\n",
    "               tp = cm[0][0]\n",
    "               fn = cm[0][1]\n",
    "               fp = cm[1][0]\n",
    "               tn = cm[1][1]\n",
    "               tpr = tp/float(tp+fn)\n",
    "               fpr = fp/float(fp+tn)\n",
    "               fnr = fn/float(fn+tp)\n",
    "               tnr = tn/float(tn+fp)\n",
    "               precision = tp/float(tp+fp)\n",
    "               recall = tp/float(tp+fn)\n",
    "               specificity = tn/float(tn+fp)\n",
    "               f1 = 2*float(precision*recall)/float(precision+recall)\n",
    "               accuracy = accuracy_score(_y2, predicted)\n",
    "               \n",
    "         \n",
    "               print('FOLD {} results:'.format(fold_number))\n",
    "               print('TP: {}, TN: {}, FP: {}, FN: {}'.format(tp,tn,fp,fn))\n",
    "               print('TPR: {}, TNR: {}, FPR: {}, FNR: {}'.format(\n",
    "                                                        tpr,tnr,fpr,fnr))   \n",
    "               print('Sensitivity/Recall: {}'.format(recall))\n",
    "               print('Specificity: {}'.format(specificity))\n",
    "               print('Precision: {}'.format(precision))\n",
    "               print('F1-measure: {}'.format(f1))\n",
    "               print('Accuracy: {}'.format(accuracy))\n",
    "               fold_number += 1\n",
    "               \n",
    "               # Store the metrics for this epoch\n",
    "               sensitivities.append(tp/float(tp+fn))\n",
    "               specificities.append(tn/float(tn+fp))\n",
    "               fars.append(fpr)\n",
    "               mdrs.append(fnr)\n",
    "               accuracies.append(accuracy)\n",
    "\n",
    "    if do_training:\n",
    "        h5features = h5py.File(features_file, 'r')\n",
    "        h5labels = h5py.File(labels_file, 'r')\n",
    "        \n",
    "        # X_full will contain all the feature vectors extracted\n",
    "        # from optical flow images\n",
    "        X_full = h5features[features_key]\n",
    "        _y_full = np.asarray(h5labels[labels_key])\n",
    "        \n",
    "        zeroes_full = np.asarray(np.where(_y_full==0)[0])\n",
    "        ones_full = np.asarray(np.where(_y_full==1)[0])\n",
    "        zeroes_full.sort()\n",
    "        ones_full.sort()\n",
    "        \n",
    "        # Use a 5 fold cross-validation\n",
    "        kf_falls = KFold(n_splits=5, shuffle=True)\n",
    "        kf_falls.get_n_splits(X_full[zeroes_full, ...])\n",
    "        \n",
    "        kf_nofalls = KFold(n_splits=5, shuffle=True)\n",
    "        kf_nofalls.get_n_splits(X_full[ones_full, ...])        \n",
    "\n",
    "        sensitivities = []\n",
    "        specificities = []\n",
    "        fars = []\n",
    "        mdrs = []\n",
    "        accuracies = []\n",
    "            \n",
    "        fold_number = 1\n",
    "        # CROSS-VALIDATION: Stratified partition of the dataset into\n",
    "        # train/test sets\n",
    "        for ((train_index_falls, test_index_falls),\n",
    "            (train_index_nofalls, test_index_nofalls)) in zip(\n",
    "                 kf_falls.split(X_full[zeroes_full, ...]),\n",
    "                 kf_nofalls.split(X_full[ones_full, ...])\n",
    "            ):\n",
    "\n",
    "            train_index_falls = np.asarray(train_index_falls)\n",
    "            test_index_falls = np.asarray(test_index_falls)\n",
    "            train_index_nofalls = np.asarray(train_index_nofalls)\n",
    "            test_index_nofalls = np.asarray(test_index_nofalls)\n",
    "\n",
    "            X = np.concatenate((X_full[zeroes_full, ...][train_index_falls, ...],\n",
    "                                X_full[ones_full, ...][train_index_nofalls, ...]))\n",
    "            _y = np.concatenate((_y_full[zeroes_full, ...][train_index_falls, ...],\n",
    "                                 _y_full[ones_full, ...][train_index_nofalls, ...]))\n",
    "            X2 = np.concatenate((X_full[zeroes_full, ...][test_index_falls, ...],\n",
    "                                 X_full[ones_full, ...][test_index_nofalls, ...]))\n",
    "            _y2 = np.concatenate((_y_full[zeroes_full, ...][test_index_falls, ...],\n",
    "                                  _y_full[ones_full, ...][test_index_nofalls, ...]))   \n",
    "\n",
    "            # Create a validation subset from the training set\n",
    "            val_size = 100\n",
    "            zeroes = np.asarray(np.where(_y==0)[0])\n",
    "            ones = np.asarray(np.where(_y==1)[0])\n",
    "            \n",
    "            zeroes.sort()\n",
    "            ones.sort()\n",
    "\n",
    "            trainval_split_0 = StratifiedShuffleSplit(n_splits=1,\n",
    "                                                   test_size=int(val_size/2),\n",
    "                                                   random_state=7)\n",
    "            indices_0 = trainval_split_0.split(X[zeroes,...],\n",
    "                                             np.argmax(_y[zeroes,...], 1))\n",
    "            trainval_split_1 = StratifiedShuffleSplit(n_splits=1,\n",
    "                                                   test_size=int(val_size/2),\n",
    "                                                   random_state=7)\n",
    "            indices_1 = trainval_split_1.split(X[ones,...],\n",
    "                                             np.argmax(_y[ones,...], 1))\n",
    "            train_indices_0, val_indices_0 = next(indices_0)\n",
    "            train_indices_1, val_indices_1 = next(indices_1)\n",
    "\n",
    "            X_train = np.concatenate([X[zeroes,...][train_indices_0,...],\n",
    "                                      X[ones,...][train_indices_1,...]],axis=0)\n",
    "            y_train = np.concatenate([_y[zeroes,...][train_indices_0,...],\n",
    "                                      _y[ones,...][train_indices_1,...]],axis=0)\n",
    "            X_val = np.concatenate([X[zeroes,...][val_indices_0,...],\n",
    "                                      X[ones,...][val_indices_1,...]],axis=0)\n",
    "            y_val = np.concatenate([_y[zeroes,...][val_indices_0,...],\n",
    "                                      _y[ones,...][val_indices_1,...]],axis=0)\n",
    "           \n",
    "\n",
    "            # Balance the number of positive and negative samples so that\n",
    "            # there is the same amount of each of them\n",
    "            all0 = np.asarray(np.where(y_train==0)[0])\n",
    "            all1 = np.asarray(np.where(y_train==1)[0])  \n",
    "            \n",
    "            if len(all0) < len(all1):\n",
    "                all1 = np.random.choice(all1, len(all0), replace=False)\n",
    "            else:\n",
    "                all0 = np.random.choice(all0, len(all1), replace=False)\n",
    "            allin = np.concatenate((all0.flatten(),all1.flatten()))\n",
    "            allin.sort()\n",
    "            X_train = X_train[allin,...]\n",
    "            y_train = y_train[allin]\n",
    "            \n",
    "            all0 = np.asarray(np.where(y_train==0)[0])\n",
    "            all1 = np.asarray(np.where(y_train==1)[0])\n",
    "            \n",
    "            # ==================== CLASSIFIER ========================\n",
    "            extracted_features = Input(shape=(num_features,),\n",
    "                                       dtype='float32', name='input')\n",
    "            if batch_norm:\n",
    "                x = BatchNormalization(axis=-1, momentum=0.99,\n",
    "                                       epsilon=0.001)(extracted_features)\n",
    "                x = Activation('relu')(x)\n",
    "            else:\n",
    "                x = ELU(alpha=1.0)(extracted_features)\n",
    "           \n",
    "            x = Dropout(0.9)(x)\n",
    "            x = Dense(4096, name='fc2', kernel_initializer='glorot_uniform')(x)\n",
    "            if batch_norm:\n",
    "                x = BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001)(x)\n",
    "                x = Activation('relu')(x)\n",
    "            else:\n",
    "                x = ELU(alpha=1.0)(x)\n",
    "            x = Dropout(0.8)(x)\n",
    "            x = Dense(1, name='predictions',\n",
    "                        kernel_initializer='glorot_uniform')(x)\n",
    "            x = Activation('sigmoid')(x)\n",
    "            \n",
    "            classifier = Model(input=extracted_features,\n",
    "                               output=x, name='classifier')\n",
    "            fold_best_model_path = best_model_path + 'urfd_fold_{}.h5'.format(\n",
    "                                                                fold_number)\n",
    "            classifier.compile(optimizer=adam, loss='binary_crossentropy',\n",
    "                               metrics=['accuracy'])\n",
    "\n",
    "            if not use_checkpoint:\n",
    "                # ==================== TRAINING ========================     \n",
    "                # weighting of each class: only the fall class gets\n",
    "                # a different weight\n",
    "                class_weight = {0: weight_0, 1: 1}\n",
    "\n",
    "                # callback definition\n",
    "                metric = 'val_loss'\n",
    "                e = EarlyStopping(monitor=metric, min_delta=0, patience=100,\n",
    "                                  mode='auto')\n",
    "                c = ModelCheckpoint(fold_best_model_path, monitor=metric,\n",
    "                                    save_best_only=True,\n",
    "                                    save_weights_only=False, mode='auto')\n",
    "                callbacks = [e, c]\n",
    "\n",
    "                # Batch training\n",
    "                if mini_batch_size == 0:\n",
    "                        history = classifier.fit(X_train, y_train, \n",
    "                                                validation_data=(X_val,y_val),\n",
    "                                                batch_size=X_train.shape[0],\n",
    "                                                nb_epoch=epochs,\n",
    "                                                shuffle='batch',\n",
    "                                                class_weight=class_weight,\n",
    "                                                callbacks=callbacks)\n",
    "                else:\n",
    "                        history = classifier.fit(X_train, y_train, \n",
    "                                                validation_data=(X_val,y_val),\n",
    "                                                batch_size=mini_batch_size,\n",
    "                                                nb_epoch=epochs,\n",
    "                                                shuffle='batch',\n",
    "                                                class_weight=class_weight,\n",
    "                                                callbacks=callbacks)\n",
    "\n",
    "                plot_training_info(plots_folder + exp, ['accuracy', 'loss'],\n",
    "                                   save_plots, history.history)\n",
    "\n",
    "                classifier = load_model(fold_best_model_path)\n",
    "\n",
    "                # Use full training set (training+validation)\n",
    "                X_train = np.concatenate((X_train, X_val), axis=0)\n",
    "                y_train = np.concatenate((y_train, y_val), axis=0)\n",
    "\n",
    "                if mini_batch_size == 0:\n",
    "                        history = classifier.fit(X_train, y_train, \n",
    "                                                batch_size=X_train.shape[0],\n",
    "                                                nb_epoch=1,\n",
    "                                                shuffle='batch',\n",
    "                                                class_weight=class_weight)\n",
    "                else:\n",
    "                        history = classifier.fit(X_train, y_train, \n",
    "                                                batch_size=mini_batch_size,\n",
    "                                                nb_epoch=1,\n",
    "                                                shuffle='batch',\n",
    "                                                class_weight=class_weight)\n",
    "\n",
    "                classifier.save(fold_best_model_path)\n",
    "\n",
    "            # ==================== EVALUATION ========================     \n",
    "            \n",
    "            # Load best model\n",
    "            print('Model loaded from checkpoint')\n",
    "            classifier = load_model(fold_best_model_path)\n",
    "   \n",
    "            if compute_metrics:\n",
    "               predicted = classifier.predict(np.asarray(X2))\n",
    "               for i in range(len(predicted)):\n",
    "                   if predicted[i] < threshold:\n",
    "                       predicted[i] = 0\n",
    "                   else:\n",
    "                       predicted[i] = 1\n",
    "               # Array of predictions 0/1\n",
    "               predicted = np.asarray(predicted).astype(int)   \n",
    "               # Compute metrics and print them\n",
    "               cm = confusion_matrix(_y2, predicted,labels=[0,1])\n",
    "               tp = cm[0][0]\n",
    "               fn = cm[0][1]\n",
    "               fp = cm[1][0]\n",
    "               tn = cm[1][1]\n",
    "               tpr = tp/float(tp+fn)\n",
    "               fpr = fp/float(fp+tn)\n",
    "               fnr = fn/float(fn+tp)\n",
    "               tnr = tn/float(tn+fp)\n",
    "               precision = tp/float(tp+fp)\n",
    "               recall = tp/float(tp+fn)\n",
    "               specificity = tn/float(tn+fp)\n",
    "               f1 = 2*float(precision*recall)/float(precision+recall)\n",
    "               accuracy = accuracy_score(_y2, predicted)\n",
    "               \n",
    "               print('FOLD {} results:'.format(fold_number))\n",
    "               print('TP: {}, TN: {}, FP: {}, FN: {}'.format(tp,tn,fp,fn))\n",
    "               print('TPR: {}, TNR: {}, FPR: {}, FNR: {}'.format(\n",
    "                                                        tpr,tnr,fpr,fnr))   \n",
    "               print('Sensitivity/Recall: {}'.format(recall))\n",
    "               print('Specificity: {}'.format(specificity))\n",
    "               print('Precision: {}'.format(precision))\n",
    "               print('F1-measure: {}'.format(f1))\n",
    "               print('Accuracy: {}'.format(accuracy))\n",
    "               fold_number += 1\n",
    "               \n",
    "               # Store the metrics for this epoch\n",
    "               sensitivities.append(tp/float(tp+fn))\n",
    "               specificities.append(tn/float(tn+fp))\n",
    "               fars.append(fpr)\n",
    "               mdrs.append(fnr)\n",
    "               accuracies.append(accuracy)\n",
    "\n",
    "        print('5-FOLD CROSS-VALIDATION RESULTS ===================')\n",
    "        print(\"Sensitivity: %.2f%% (+/- %.2f%%)\" % (np.mean(sensitivities),\n",
    "                                                         np.std(sensitivities)))\n",
    "        print(\"Specificity: %.2f%% (+/- %.2f%%)\" % (np.mean(specificities),\n",
    "                                                         np.std(specificities)))\n",
    "        print(\"FAR: %.2f%% (+/- %.2f%%)\" % (np.mean(fars), np.std(fars)))\n",
    "        print(\"MDR: %.2f%% (+/- %.2f%%)\" % (np.mean(mdrs), np.std(mdrs)))\n",
    "        print(\"Accuracy: %.2f%% (+/- %.2f%%)\" % (np.mean(accuracies),\n",
    "                                                         np.std(accuracies)))\n",
    "\"\"\"        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
